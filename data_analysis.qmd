---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
author:
  - name: Connor Coulter
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
  - name: Wei Wang
    affiliations:
      - ref: bu
  - name: Balqis Bevi Abdul Hannan Kanaga
    affiliations:
      - ref: bu
bibliography: references.bib
csl: csl/econometrica.csl
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
---
# Import Data

Importing dataset using PySpark. The code shows the schema and first few rows of the dataset. Schema is shown below but partial dataframe is muted.

```{python}

from pyspark.sql import SparkSession

# Start a Spark session
spark = SparkSession.builder.appName("JobPostingsAnalysis").getOrCreate()

# Load the CSV file into a Spark DataFrame
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("lightcast_job_postings.csv")

# Show schema
df.printSchema()
```

```{python}
#| echo: false
df.show(5, truncate=False)
```

# Data Cleaning & Preprocessing
## Dropping Unnecessary Columns
The raw dataset contains numerous fields, many of which are either redundant, outdated, or not useful for the scope of our analysis.

We dropped the following types of columns:

- **Tracking fields**: Metadata such as `ID`, `URL`, `ACTIVE_URLS`, `DUPLICATES`, and `LAST_UPDATED_TIMESTAMP` are not analytically meaningful.
- **Multiple versions of NAICS and SOC codes**: To avoid redundancy, we retained only the **most current versions** (`NAICS_2022_6` and `SOC_2021_4`), which reflect the latest industry and occupational classifications.
- **Unstructured text fields**: Fields like `BODY` contain long, unstructured descriptions not used in this stage of analysis.
- **Internal mapping columns**: Columns such as `CIP2`, `CIP4`, and `LOT_*` relate to alternate classification systems that were not relevant to our focus.


```{python}
columns_to_drop = [
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    "TITLE_RAW", "COMPANY_RAW", "BODY", "ACTIVE_SOURCES_INFO",
    "NAICS2", "NAICS2_NAME", "NAICS3", "NAICS3_NAME", "NAICS4", "NAICS4_NAME",
    "NAICS5", "NAICS5_NAME", "NAICS6", "NAICS6_NAME",
    "SOC_2", "SOC_2_NAME", "SOC_3", "SOC_3_NAME", "SOC_5", "SOC_5_NAME",
    "CIP2", "CIP2_NAME", "CIP4", "CIP4_NAME", "CIP6", "CIP6_NAME",
    "LOT_CAREER_AREA", "LOT_CAREER_AREA_NAME", "LOT_OCCUPATION", "LOT_OCCUPATION_NAME",
    "LOT_SPECIALIZED_OCCUPATION", "LOT_SPECIALIZED_OCCUPATION_NAME",
    "LOT_OCCUPATION_GROUP", "LOT_OCCUPATION_GROUP_NAME",
    "LOT_V6_SPECIALIZED_OCCUPATION", "LOT_V6_SPECIALIZED_OCCUPATION_NAME",
    "LOT_V6_OCCUPATION", "LOT_V6_OCCUPATION_NAME", "LOT_V6_OCCUPATION_GROUP",
    "LOT_V6_OCCUPATION_GROUP_NAME", "LOT_V6_CAREER_AREA", "LOT_V6_CAREER_AREA_NAME",
    "ONET", "ONET_NAME", "ONET_2019", "ONET_2019_NAME"
]
df.drop(columns=columns_to_drop, inplace=True)
```

By dropping these columns, we streamline the dataset for cleaner visualizations, reduce noise in statistical summaries, and maintain a focus on relevant dimensions like title, salary, industry, remote status, and location.

## Handling Missing Values
To address incomplete records, we applied a **column-wise and row-wise strategy**:
- Columns with more than 50% missing values were removed entirely.
- For numerical fields (e.g., Salary), we imputed missing values using the median, which is robust to outliers.
- For categorical fields (e.g., Industry or Remote Type), we used "Unknown" as a placeholder.
We also visualized the missing value distribution using missingno.
```{python}
import missingno as msno
import matplotlib.pyplot as plt

# Visualize missing data
msno.heatmap(df)
plt.title("Missing Values Heatmap")
plt.show()

# Drop columns with >50% missing values
df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)

# Fill missing numerical values
df["Salary"].fillna(df["Salary"].median(), inplace=True)

# Fill missing categorical values
df["INDUSTRY"] = df.get("INDUSTRY", df.get("INDUSTRY_NAME", "Unknown"))
df["INDUSTRY"].fillna("Unknown", inplace=True)

df["REMOTE_TYPE_NAME"].fillna("Unknown", inplace=True)
```

## Remove Duplicates
To avoid inflated job counts, we identified and removed exact duplicates using the combination of TITLE, COMPANY_NAME, LOCATION, and POSTED.
```{python}
df.drop_duplicates(
    subset=["TITLE", "COMPANY_NAME", "LOCATION", "POSTED"],
    keep="first",
    inplace=True
)
```