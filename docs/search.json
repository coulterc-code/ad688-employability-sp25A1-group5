[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction: Group 5",
    "section": "",
    "text": "Team: Connor Coulter, Wei Wang, Balqis Bevi Abdul Hannan Kanaga\nTopic: AI vs. Non-AI Job Growth — Is AI taking over or creating more jobs?\nCourse: AD 688 – Cloud Analytics for Business\nThis site hosts our research rationale, intro, and literature review for Project Selection I, II and III."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Highlights sectors where demand is concentrated, showing which industries are actively hiring.\n\n\n\n\nTop Hiring Industries: Custom Computer Programming, Management Consulting, and Employment Agencies dominate job postings.\nSkewed Distribution: The top 4 industries account for a significantly larger share of job postings than the rest.\nProfessional Services Focus: Many high-posting sectors are centered around tech, consulting, healthcare and education — reflecting demand for knowledge-based roles."
  },
  {
    "objectID": "eda.html#rationale",
    "href": "eda.html#rationale",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Highlights sectors where demand is concentrated, showing which industries are actively hiring."
  },
  {
    "objectID": "eda.html#key-insights",
    "href": "eda.html#key-insights",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Top Hiring Industries: Custom Computer Programming, Management Consulting, and Employment Agencies dominate job postings.\nSkewed Distribution: The top 4 industries account for a significantly larger share of job postings than the rest.\nProfessional Services Focus: Many high-posting sectors are centered around tech, consulting, healthcare and education — reflecting demand for knowledge-based roles."
  },
  {
    "objectID": "eda.html#rationale-1",
    "href": "eda.html#rationale-1",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.1 Rationale",
    "text": "2.1 Rationale\nShows where negotiation power exists and highlights industries paying well."
  },
  {
    "objectID": "eda.html#key-insights-1",
    "href": "eda.html#key-insights-1",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.2 Key Insights",
    "text": "2.2 Key Insights\n\nWide Salary Ranges in Staffing & Tech Services: Industries like Temporary Help Services and Employment Placement Agencies exhibit large salary spreads with high outliers, though their median pay remains modest.\nStable Pay in Professional Sectors: Most industries maintain a consistent median salary around $100K–$150K, reflecting standardized compensation and less variation in negotiation power."
  },
  {
    "objectID": "eda.html#rationale-2",
    "href": "eda.html#rationale-2",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "3.1 Rationale",
    "text": "3.1 Rationale\nWorkplace flexibility is a major factor in today’s job market."
  },
  {
    "objectID": "eda.html#key-insights-2",
    "href": "eda.html#key-insights-2",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "3.2 Key Insights",
    "text": "3.2 Key Insights\n\nLimited Remote Availability: Only about 17% of job postings are labeled as Remote, with Hybrid Remote and Not Remote making up even smaller portions.\nData Gaps in Job Listings: A significant 78.3% of postings lack remote classification, indicating either incomplete employer data or inconsistent labeling, which may affect job seekers’ filtering and selection."
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "1 Group 5 skill level\n\n\n\n\n\n\n\n\n\n\n\n2 Compare our group’s skills against job market demand\n\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\nDocker\nAWS\n\n\nName\n\n\n\n\n\n\n\n\n\n\nConnor\n2\n2\n2\n2\n0\n0\n\n\nWei\n1\n2\n1\n1\n0\n0\n\n\nBalqis\n3\n4\n2\n2\n0\n0\n\n\n\n\n\n\n\n\n\n3 Improvement Plan\n\nBalqis: Her Machine Learning and Cloud Computing are at a basic level, leaving room to grow. With a career in data analysis and visualization, Machine Learning isn’t her top priority, but Cloud Computing is worth developing further. Strengthening Python would also be valuable, as it’s essential for data analysts. A good approach is to sharpen her skills through small personal projects and apply what she learns at work. If her fundamentals feel solid, she can move towards certifications.\nWei: Her Python and Machine Learning are at a basic level, so she has the option to develop them further depending on how relevant they are to her career path. Since her SQL is already stronger, focusing on Python would be the most practical next step if she chooses to continue building technical skills. A good approach is to take it gradually through small projects and applied practice, and then expand into more advanced areas only if it fits her goals.\nConnor: His skills are fairly even across all areas, at a basic stage, which gives him room to build depth. Bumping Python up to a stronger level would give him the most flexibility, while also continuing to grow in Cloud Computing to keep pace with current tools and workflows. A steady way forward is to practice Python through hands-on work and then bring in cloud tools as he becomes more confident."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "",
    "text": "This project explores AI vs Non-AI careers using the lightcast_job_postings.csv dataset.\nWe apply clustering, regression, and classification to evaluate trends in job markets, with a focus on salary, experience, and employability.\nThe goal is to help job seekers understand how AI is shaping opportunities in 2024."
  },
  {
    "objectID": "project4.html#introduction",
    "href": "project4.html#introduction",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "",
    "text": "This project explores AI vs Non-AI careers using the lightcast_job_postings.csv dataset.\nWe apply clustering, regression, and classification to evaluate trends in job markets, with a focus on salary, experience, and employability.\nThe goal is to help job seekers understand how AI is shaping opportunities in 2024."
  },
  {
    "objectID": "project4.html#data-preparation",
    "href": "project4.html#data-preparation",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "Data Preparation",
    "text": "Data Preparation\nimport pandas as pd, numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, LogisticRegression from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, confusion_matrix from sklearn.impute import SimpleImputer from sklearn.cluster import KMeans import plotly.express as px, plotly.io as pio import plotly.figure_factory as ff\npio.templates.default = “plotly_white”"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI vs. Non-AI Job Growth",
    "section": "",
    "text": "Introduction\nArtificial intelligence (AI) is changing the working world, affecting jobs and livelihood. This makes AI an important topic to study. Jobs that are related to AI, such as data science, robotics, and machine learning, are becoming more common, often paying well. Simultaneously, traditional jobs such as farming, retail, or office work are slowly being reduced and taken over due to automation and new advanced technology taking over. This does not imply that every job disappears; however, it does mean that the job market is shifting alongside AI, and people are starting to worry that their future careers might not be as secure as before.\nIn 2024, people are worried about job security as many industries are already adopting AI tools. Some see AI as a threat to their jobs, while others see it as a chance to move into new roles. Looking at AI versus non-AI careers will help us further understand how and where job opportunities are growing, which areas are shrinking, and what skills people may need to adapt in the future.\n\n\nResearch Rationale\nThis topic is not only important, but also interesting to understand because it highlights how the progress of AI can actually eliminate and create jobs at the same time. In 2024, many companies are integrating AI, accelerating changes in the job market. Automation is replacing repetitive tasks, while AI-driven roles are much higher in demand. However, this transition is not equal.\nAI affects people differently depending on their job, skill level, and where they live. For example, store cashiers are being replaced by self-checkout machines, but new jobs like AI engineers are being created to build and maintain those systems. The problem is that it might take five people to run a store, but an entire company may only need a few engineers to build the technology that replaces these cashiers. This means that some workers may lose jobs unless they learn new skills (see e.g., Tiwari ((2023))).\n\n\nPreliminary Discussion\nAI has already shown a large positive impact: it reduces the need for workers in routine tasks while simultaneously creating new job roles like AI ethics officers and prompt engineers. These jobs did not exist a decade ago, which shows that AI is not necessarily taking away jobs but also opening new career paths.\nThe real issue is that the shift is uneven. It is not always a fair trade when losing one job and gaining another because some countries with strong economies may be able to support workers with training programs, but poorer countries may not. It should also be noted that there will be workers who are not interested in technological roles. This shows that AI’s impact depends on economics, training opportunities, and even personal preference and interest. In some settings AI tools raise worker productivity—especially for newer workers—rather than replace them outright (Brynjolfsson, Li, and Raymond ((2023))); employers are also placing more value on complementary human skills (teamwork, adaptability, digital literacy) even as purely routine tasks decline (Mäkelä and Stephany ((2024))).\n\n\nLiterature Review\nResearchers have studied how AI is affecting jobs, and findings vary. Tiwari ((2023)) explains that while AI/ML can replace some roles—especially repetitive, routine work—they also create opportunities in areas like data analysis and AI system development. The key challenge is reskilling: workers’ ability to adapt to new roles determines whether they benefit.\nOther studies show that AI can help workers instead of only replacing them. Brynjolfsson, Li, and Raymond ((2023)) find that customer service workers, especially new ones, became more productive when using AI tools alongside normal tasks. Mäkelä and Stephany ((2024)) analyze job ads across countries and find demand rising for human, complementary skills (teamwork, adaptability, digital literacy), while demand falls for narrowly routine tasks. Together, these findings suggest AI’s effects are heterogeneous: outcomes vary by occupation, tasks, skills, and context.\n\n\n\n\n\n\n\n\nReferences\n\nBrynjolfsson, E., D. Li, and L. Raymond. (2023): “Generative AI at work,” arXiv preprint arXiv:2304.11771,.\n\n\nMäkelä, E., and F. Stephany. (2024): “Complement or substitute? How AI increases the demand for human skills,” arXiv preprint arXiv:2412.19754,.\n\n\nTiwari, R. (2023): “The impact of AI and machine learning on job displacement and employment opportunities,” International Journal of Engineering Technologies and Management Research, 7, 1–9."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "Loaded dataset: (72498, 131)\n\n\n/var/folders/g7/sfc5tly50013vn_cy1c842180000gn/T/ipykernel_48814/4082786718.py:32: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object."
  },
  {
    "objectID": "data_cleaning.html#drop-unnecessary-columns",
    "href": "data_cleaning.html#drop-unnecessary-columns",
    "title": "Data Cleaning & Exploration",
    "section": "2.1 Drop Unnecessary Columns",
    "text": "2.1 Drop Unnecessary Columns\n\n\nDerived non-null: {'INDUSTRY_DISPLAY': np.int64(72454), 'SALARY_DISPLAY': np.int64(72498)}"
  },
  {
    "objectID": "data_cleaning.html#drop-unnecessary-columns-1",
    "href": "data_cleaning.html#drop-unnecessary-columns-1",
    "title": "Data Cleaning & Exploration",
    "section": "2.2 Drop Unnecessary Columns",
    "text": "2.2 Drop Unnecessary Columns\n\n\nRemaining columns (first 30): ['LAST_UPDATED_DATE', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION']"
  },
  {
    "objectID": "data_cleaning.html#handle-missing-values",
    "href": "data_cleaning.html#handle-missing-values",
    "title": "Data Cleaning & Exploration",
    "section": "2.3 Handle Missing Values",
    "text": "2.3 Handle Missing Values\n\n\n\n\n\n\n\n\n\n/var/folders/g7/sfc5tly50013vn_cy1c842180000gn/T/ipykernel_48814/2447604449.py:10: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n/var/folders/g7/sfc5tly50013vn_cy1c842180000gn/T/ipykernel_48814/2447604449.py:13: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object."
  },
  {
    "objectID": "data_cleaning.html#remove-duplicates",
    "href": "data_cleaning.html#remove-duplicates",
    "title": "Data Cleaning & Exploration",
    "section": "2.4 Remove Duplicates",
    "text": "2.4 Remove Duplicates\n\n\nRemoved 3300 duplicates using ['TITLE', 'COMPANY_NAME', 'LOCATION', 'POSTED']"
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "1 Import Data\nimport pandas as pd, numpy as np, os\nCSV_PATHS = [“data/lightcast_job_postings.csv”, “lightcast_job_postings.csv”] csv_path = next((p for p in CSV_PATHS if os.path.exists(p)), None) if not csv_path: raise FileNotFoundError(“⚠️ lightcast_job_postings.csv not found”)\ndf = pd.read_csv(csv_path, low_memory=False) print(“Loaded dataset:”, df.shape) df.head(5)\n\n\n2 Derive helper columns\ndf[“INDUSTRY_DISPLAY”] = ( df[“NAICS_2022_6_NAME”] if “NAICS_2022_6_NAME” in df.columns else df.get(“INDUSTRY”, pd.Series([“Unknown”]*len(df))) )\nsalary_candidates = [“SALARY”,“SALARY_MEDIAN”,“SALARY_MID”,“SALARY_ANNUAL”,“PAY_RATE”] for c in salary_candidates: if c in df.columns: df[c] = pd.to_numeric(df[c], errors=“coerce”)\ndf[“SALARY_DISPLAY”] = next( (df[c] for c in salary_candidates if c in df.columns), pd.Series([np.nan]*len(df)) )\nprint(“Derived non-null:”, { “INDUSTRY_DISPLAY”: df[“INDUSTRY_DISPLAY”].notna().sum(), “SALARY_DISPLAY”: df[“SALARY_DISPLAY”].notna().sum() }) Data Cleaning & Preprocessing Drop Unnecessary Columns columns_to_drop = [ “ID”,“LAST_UPDATED_TIMESTAMP”,“DUPLICATES”,“ACTIVE_URLS”,“ACTIVE_SOURCES_INFO”, “TITLE_RAW”,“BODY”,“COMPANY_RAW”, “NAICS2”,“NAICS2_NAME”,“NAICS3”,“NAICS3_NAME”,“NAICS4”,“NAICS4_NAME”, “NAICS5”,“NAICS5_NAME”,“NAICS6”,“NAICS6_NAME”, “NAICS_2022_2”,“NAICS_2022_2_NAME”,“NAICS_2022_3”,“NAICS_2022_3_NAME”, “NAICS_2022_4”,“NAICS_2022_4_NAME”,“NAICS_2022_5”,“NAICS_2022_5_NAME”, “SOC_2”,“SOC_2_NAME”,“SOC_3”,“SOC_3_NAME”,“SOC_5”,“SOC_5_NAME”, “CIP2”,“CIP2_NAME”,“CIP4”,“CIP4_NAME”,“CIP6”,“CIP6_NAME”, “LOT_CAREER_AREA”,“LOT_CAREER_AREA_NAME”,“LOT_OCCUPATION”,“LOT_OCCUPATION_NAME”, “LOT_SPECIALIZED_OCCUPATION”,“LOT_SPECIALIZED_OCCUPATION_NAME”, “LOT_OCCUPATION_GROUP”,“LOT_OCCUPATION_GROUP_NAME”, “LOT_V6_SPECIALIZED_OCCUPATION”,“LOT_V6_SPECIALIZED_OCCUPATION_NAME”, “LOT_V6_OCCUPATION”,“LOT_V6_OCCUPATION_NAME”,“LOT_V6_OCCUPATION_GROUP”, “LOT_V6_OCCUPATION_GROUP_NAME”,“LOT_V6_CAREER_AREA”,“LOT_V6_CAREER_AREA_NAME”, “ONET”,“ONET_NAME”,“ONET_2019”,“ONET_2019_NAME”] drop_existing = [c for c in columns_to_drop if c in df.columns] df.drop(columns=drop_existing, inplace=True) print(“Remaining columns (first 30):”, list(df.columns)[:30])\nHandle Missing Values import missingno as msno, matplotlib.pyplot as plt\nmsno.heatmap(df) plt.title(“Missing Values Heatmap”) plt.show()\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\nif “SALARY_DISPLAY” in df.columns: df[“SALARY_DISPLAY”].fillna(df[“SALARY_DISPLAY”].median(), inplace=True)\nfor col in df.select_dtypes(include=“object”).columns: df[col].fillna(“Unknown”, inplace=True)\nRemove Duplicates subset_cols = [c for c in [“TITLE”,“COMPANY_NAME”,“LOCATION”,“POSTED”] if c in df.columns] if subset_cols: before = len(df) df.drop_duplicates(subset=subset_cols, keep=“first”, inplace=True) print(f”Removed {before - len(df)} duplicates using {subset_cols}“)\nExploratory Data Analysis (EDA) Job Postings by Industry (Top 15) import plotly.express as px\ncounts = ( df[“INDUSTRY_DISPLAY”] .value_counts(dropna=False) .head(15) .reset_index(name=“Count”) .rename(columns={“index”: “Industry”}) .sort_values(“Count”) ) fig1 = px.bar( counts, x=“Count”, y=“Industry”, orientation=“h”, title=“Top 15 Industries by Number of Job Postings” ) fig1.show()\nSalary Distribution by Industry (Top 15) sdf = df[[“INDUSTRY_DISPLAY”,“SALARY_DISPLAY”]].copy() sdf = sdf.dropna() sdf = sdf[sdf[“SALARY_DISPLAY”] &gt; 0]\ntop_industries = sdf[“INDUSTRY_DISPLAY”].value_counts().head(15).index sdf = sdf[sdf[“INDUSTRY_DISPLAY”].isin(top_industries)]\nfig2 = px.box( sdf, x=“INDUSTRY_DISPLAY”, y=“SALARY_DISPLAY”, title=“Salary Distribution by Industry (Top 15)”, points=False ) fig2.update_layout(xaxis_tickangle=-45) fig2.show()\nRemote vs. On-Site Jobs if “REMOTE_TYPE_NAME” in df.columns: rc = df[“REMOTE_TYPE_NAME”].value_counts().reset_index() rc.columns = [“Remote Type”,“Count”] fig3 = px.pie( rc, names=“Remote Type”, values=“Count”, title=“Remote vs. On-Site Job Distribution” ) fig3.show()\nEDA: Rationale & Insights Job Postings by Industry\nWhy: Highlights sectors where demand is concentrated, showing which industries are actively hiring. Key Insights: The top three industries by job postings are Temporary Help Services, Miscellaneous Ambulatory Health Care Services, and Semiconductor and Related Device Manufacturing.\nSalary Distribution by Industry\nWhy: Shows where negotiation power exists and highlights industries paying well. Key Insights: Automotive Parts and Accessories Retailers show a wide range (negotiation potential), while Barber Shops show a narrow range (little negotiation).\nRemote vs. On-Site Jobs\nWhy: Workplace flexibility is a major factor in today’s job market. Key Insights: Most postings (78.3%) don’t specify remote status. About 17% are remote, 3.1% hybrid, and 1.6% explicitly not remote."
  },
  {
    "objectID": "project4.html#regression-salary-prediction",
    "href": "project4.html#regression-salary-prediction",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "Regression: Salary Prediction",
    "text": "Regression: Salary Prediction\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[8], line 17\n     14 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n     16 reg = LinearRegression()\n---&gt; 17 reg.fit(X_train, y_train)\n     18 y_pred = reg.predict(X_test)\n     20 rmse = mean_squared_error(y_test, y_pred, squared=False)\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/base.py:1365, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)\n   1358     estimator._validate_params()\n   1360 with config_context(\n   1361     skip_parameter_validation=(\n   1362         prefer_skip_nested_validation or global_skip_validation\n   1363     )\n   1364 ):\n-&gt; 1365     return fit_method(estimator, *args, **kwargs)\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:618, in LinearRegression.fit(self, X, y, sample_weight)\n    614 n_jobs_ = self.n_jobs\n    616 accept_sparse = False if self.positive else [\"csr\", \"csc\", \"coo\"]\n--&gt; 618 X, y = validate_data(\n    619     self,\n    620     X,\n    621     y,\n    622     accept_sparse=accept_sparse,\n    623     y_numeric=True,\n    624     multi_output=True,\n    625     force_writeable=True,\n    626 )\n    628 has_sw = sample_weight is not None\n    629 if has_sw:\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2971, in validate_data(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\n   2969         y = check_array(y, input_name=\"y\", **check_y_params)\n   2970     else:\n-&gt; 2971         X, y = check_X_y(X, y, **check_params)\n   2972     out = X, y\n   2974 if not no_val_X and check_params.get(\"ensure_2d\", True):\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1368, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n   1362     raise ValueError(\n   1363         f\"{estimator_name} requires y to be passed, but the target y is None\"\n   1364     )\n   1366 ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n-&gt; 1368 X = check_array(\n   1369     X,\n   1370     accept_sparse=accept_sparse,\n   1371     accept_large_sparse=accept_large_sparse,\n   1372     dtype=dtype,\n   1373     order=order,\n   1374     copy=copy,\n   1375     force_writeable=force_writeable,\n   1376     ensure_all_finite=ensure_all_finite,\n   1377     ensure_2d=ensure_2d,\n   1378     allow_nd=allow_nd,\n   1379     ensure_min_samples=ensure_min_samples,\n   1380     ensure_min_features=ensure_min_features,\n   1381     estimator=estimator,\n   1382     input_name=\"X\",\n   1383 )\n   1385 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n   1387 check_consistent_length(X, y)\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:971, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n    966 if pandas_requires_conversion:\n    967     # pandas dataframe requires conversion earlier to handle extension dtypes with\n    968     # nans\n    969     # Use the original dtype for conversion if dtype is None\n    970     new_dtype = dtype_orig if dtype is None else dtype\n--&gt; 971     array = array.astype(new_dtype)\n    972     # Since we converted here, we do not need to convert again later\n    973     dtype = None\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/generic.py:6662, in NDFrame.astype(self, dtype, copy, errors)\n   6656     results = [\n   6657         ser.astype(dtype, copy=copy, errors=errors) for _, ser in self.items()\n   6658     ]\n   6660 else:\n   6661     # else, only a single dtype is given\n-&gt; 6662     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n   6663     res = self._constructor_from_mgr(new_data, axes=new_data.axes)\n   6664     return res.__finalize__(self, method=\"astype\")\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:430, in BaseBlockManager.astype(self, dtype, copy, errors)\n    427 elif using_copy_on_write():\n    428     copy = False\n--&gt; 430 return self.apply(\n    431     \"astype\",\n    432     dtype=dtype,\n    433     copy=copy,\n    434     errors=errors,\n    435     using_cow=using_copy_on_write(),\n    436 )\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:363, in BaseBlockManager.apply(self, f, align_keys, **kwargs)\n    361         applied = b.apply(f, **kwargs)\n    362     else:\n--&gt; 363         applied = getattr(b, f)(**kwargs)\n    364     result_blocks = extend_blocks(applied, result_blocks)\n    366 out = type(self).from_blocks(result_blocks, self.axes)\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:784, in Block.astype(self, dtype, copy, errors, using_cow, squeeze)\n    781         raise ValueError(\"Can not squeeze with more than one column.\")\n    782     values = values[0, :]  # type: ignore[call-overload]\n--&gt; 784 new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n    786 new_values = maybe_coerce_values(new_values)\n    788 refs = None\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:237, in astype_array_safe(values, dtype, copy, errors)\n    234     dtype = dtype.numpy_dtype\n    236 try:\n--&gt; 237     new_values = astype_array(values, dtype, copy=copy)\n    238 except (ValueError, TypeError):\n    239     # e.g. _astype_nansafe can fail on object-dtype of strings\n    240     #  trying to convert to float\n    241     if errors == \"ignore\":\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:182, in astype_array(values, dtype, copy)\n    179     values = values.astype(dtype, copy=copy)\n    181 else:\n--&gt; 182     values = _astype_nansafe(values, dtype, copy=copy)\n    184 # in pandas we don't store numpy str dtypes, so convert to object\n    185 if isinstance(dtype, np.dtype) and issubclass(values.dtype.type, str):\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:133, in _astype_nansafe(arr, dtype, copy, skipna)\n    129     raise ValueError(msg)\n    131 if copy or arr.dtype == object or dtype == object:\n    132     # Explicit copy, or required since NumPy can't view from / to object.\n--&gt; 133     return arr.astype(dtype, copy=True)\n    135 return arr.astype(dtype, copy=copy)\n\nValueError: could not convert string to float: '40550bdbcd862ac41ab06311a56ace12eae8927e'"
  },
  {
    "objectID": "project4.html#classification-ai-vs-non-ai-jobs",
    "href": "project4.html#classification-ai-vs-non-ai-jobs",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "Classification: AI vs Non-AI Jobs",
    "text": "Classification: AI vs Non-AI Jobs\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[9], line 13\n     10 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n     12 clf = LogisticRegression(max_iter=1000)\n---&gt; 13 clf.fit(X_train, y_train)\n     14 y_pred = clf.predict(X_test)\n     16 acc = accuracy_score(y_test, y_pred)\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/base.py:1365, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)\n   1358     estimator._validate_params()\n   1360 with config_context(\n   1361     skip_parameter_validation=(\n   1362         prefer_skip_nested_validation or global_skip_validation\n   1363     )\n   1364 ):\n-&gt; 1365     return fit_method(estimator, *args, **kwargs)\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247, in LogisticRegression.fit(self, X, y, sample_weight)\n   1244 else:\n   1245     _dtype = [np.float64, np.float32]\n-&gt; 1247 X, y = validate_data(\n   1248     self,\n   1249     X,\n   1250     y,\n   1251     accept_sparse=\"csr\",\n   1252     dtype=_dtype,\n   1253     order=\"C\",\n   1254     accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n   1255 )\n   1256 check_classification_targets(y)\n   1257 self.classes_ = np.unique(y)\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2971, in validate_data(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\n   2969         y = check_array(y, input_name=\"y\", **check_y_params)\n   2970     else:\n-&gt; 2971         X, y = check_X_y(X, y, **check_params)\n   2972     out = X, y\n   2974 if not no_val_X and check_params.get(\"ensure_2d\", True):\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1368, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n   1362     raise ValueError(\n   1363         f\"{estimator_name} requires y to be passed, but the target y is None\"\n   1364     )\n   1366 ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n-&gt; 1368 X = check_array(\n   1369     X,\n   1370     accept_sparse=accept_sparse,\n   1371     accept_large_sparse=accept_large_sparse,\n   1372     dtype=dtype,\n   1373     order=order,\n   1374     copy=copy,\n   1375     force_writeable=force_writeable,\n   1376     ensure_all_finite=ensure_all_finite,\n   1377     ensure_2d=ensure_2d,\n   1378     allow_nd=allow_nd,\n   1379     ensure_min_samples=ensure_min_samples,\n   1380     ensure_min_features=ensure_min_features,\n   1381     estimator=estimator,\n   1382     input_name=\"X\",\n   1383 )\n   1385 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n   1387 check_consistent_length(X, y)\n\nFile ~/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:971, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n    966 if pandas_requires_conversion:\n    967     # pandas dataframe requires conversion earlier to handle extension dtypes with\n    968     # nans\n    969     # Use the original dtype for conversion if dtype is None\n    970     new_dtype = dtype_orig if dtype is None else dtype\n--&gt; 971     array = array.astype(new_dtype)\n    972     # Since we converted here, we do not need to convert again later\n    973     dtype = None\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/generic.py:6662, in NDFrame.astype(self, dtype, copy, errors)\n   6656     results = [\n   6657         ser.astype(dtype, copy=copy, errors=errors) for _, ser in self.items()\n   6658     ]\n   6660 else:\n   6661     # else, only a single dtype is given\n-&gt; 6662     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n   6663     res = self._constructor_from_mgr(new_data, axes=new_data.axes)\n   6664     return res.__finalize__(self, method=\"astype\")\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:430, in BaseBlockManager.astype(self, dtype, copy, errors)\n    427 elif using_copy_on_write():\n    428     copy = False\n--&gt; 430 return self.apply(\n    431     \"astype\",\n    432     dtype=dtype,\n    433     copy=copy,\n    434     errors=errors,\n    435     using_cow=using_copy_on_write(),\n    436 )\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:363, in BaseBlockManager.apply(self, f, align_keys, **kwargs)\n    361         applied = b.apply(f, **kwargs)\n    362     else:\n--&gt; 363         applied = getattr(b, f)(**kwargs)\n    364     result_blocks = extend_blocks(applied, result_blocks)\n    366 out = type(self).from_blocks(result_blocks, self.axes)\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:784, in Block.astype(self, dtype, copy, errors, using_cow, squeeze)\n    781         raise ValueError(\"Can not squeeze with more than one column.\")\n    782     values = values[0, :]  # type: ignore[call-overload]\n--&gt; 784 new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n    786 new_values = maybe_coerce_values(new_values)\n    788 refs = None\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:237, in astype_array_safe(values, dtype, copy, errors)\n    234     dtype = dtype.numpy_dtype\n    236 try:\n--&gt; 237     new_values = astype_array(values, dtype, copy=copy)\n    238 except (ValueError, TypeError):\n    239     # e.g. _astype_nansafe can fail on object-dtype of strings\n    240     #  trying to convert to float\n    241     if errors == \"ignore\":\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:182, in astype_array(values, dtype, copy)\n    179     values = values.astype(dtype, copy=copy)\n    181 else:\n--&gt; 182     values = _astype_nansafe(values, dtype, copy=copy)\n    184 # in pandas we don't store numpy str dtypes, so convert to object\n    185 if isinstance(dtype, np.dtype) and issubclass(values.dtype.type, str):\n\nFile ~/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:133, in _astype_nansafe(arr, dtype, copy, skipna)\n    129     raise ValueError(msg)\n    131 if copy or arr.dtype == object or dtype == object:\n    132     # Explicit copy, or required since NumPy can't view from / to object.\n--&gt; 133     return arr.astype(dtype, copy=True)\n    135 return arr.astype(dtype, copy=copy)\n\nValueError: could not convert string to float: '40550bdbcd862ac41ab06311a56ace12eae8927e'"
  },
  {
    "objectID": "project4.html#clustering-job-segmentation",
    "href": "project4.html#clustering-job-segmentation",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "Clustering: Job Segmentation",
    "text": "Clustering: Job Segmentation\n\n\n\n\n\n\n\n\n\nsalary\nis_ai\ncluster\n\n\n\n\n4\n92500.0\n0\n1\n\n\n5\n110155.0\n0\n0\n\n\n9\n92962.0\n0\n1\n\n\n10\n107645.0\n0\n0\n\n\n13\n192800.0\n0\n2\n\n\n14\n81286.0\n0\n1\n\n\n16\n125900.0\n0\n0\n\n\n19\n170000.0\n0\n2\n\n\n20\n110155.0\n0\n0\n\n\n21\n136950.0\n0\n0"
  },
  {
    "objectID": "project4.html#visualizations",
    "href": "project4.html#visualizations",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "Visualizations",
    "text": "Visualizations"
  },
  {
    "objectID": "project4.html#interpretation",
    "href": "project4.html#interpretation",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "Interpretation",
    "text": "Interpretation\n\nRegression: shows how salary varies by industry and other features.\n\nClassification: predicts which jobs are AI-related with measurable accuracy and F1 score.\n\nClustering: segments jobs into natural groups (e.g., high-salary AI-heavy vs lower-salary non-AI).\n\nImplications for job seekers:\n- AI roles may show higher salary clusters.\n- Certain industries have stronger AI adoption.\n- Understanding these trends helps candidates position themselves competitively."
  },
  {
    "objectID": "project4.html#analysis",
    "href": "project4.html#analysis",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "Analysis",
    "text": "Analysis\nimport pandas as pd, numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, LogisticRegression from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, confusion_matrix from sklearn.impute import SimpleImputer from sklearn.cluster import KMeans import plotly.express as px, plotly.io as pio import plotly.figure_factory as ff\npio.templates.default = “plotly_white”"
  },
  {
    "objectID": "project4.html#insights-for-job-seekers",
    "href": "project4.html#insights-for-job-seekers",
    "title": "Project 4: AI vs Non-AI Careers",
    "section": "Insights for Job Seekers",
    "text": "Insights for Job Seekers\n\nAI roles often cluster at higher salaries compared to non-AI roles.\n\nExperience remains critical — higher years of experience align with higher pay clusters.\n\nIndustries with strong AI adoption (e.g., tech, finance) show clearer salary advantages.\n\n\nTakeaways:\n\nHighlight AI-related skills to access higher-paying roles.\n\nLeverage industry trends to target fields with high AI adoption.\n\nUse clustering insights to understand where your profile fits (AI-heavy vs. traditional roles)."
  }
]